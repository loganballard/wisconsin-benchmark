\documentclass[11pt,letterpaper]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage[latin1]{inputenc}
\usepackage{amsmath, amsfonts, amssymb, graphicx, enumitem, algpseudocode}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{CS587}
\chead{Benchmarking Postgres}
\rhead{Logan Ballard}
\author{Logan Ballard}
\title{Wisconsin Benchmark Pt 2: Benchmarking Postgres}
\date{05/15/2019}
\begin{document}
\maketitle

\begin{enumerate}
	
\item \textbf{Background}

Due to its ubiquity and my own familiarity with the system, I have chosen to benchmark postgres against itself.   Postgres is a very flexible and well-supported database system which has a lot of community and enterprise backing behind it.  It will behoove me to learn much mroe of its ins and outs if I wish to use it professionally, which seems very likely given how widespread its adoption is.\\

In addition to business reasons for choosing postgres, it is useful because it has a large amount of configurability.  This ability allows for near-infinite tweaking of performance to maximize different attributes of the database.  It also can be run locally without much setup, which allows for rapid iteration on various ideas.  Through tools like $pgAdmin$ and integration with familiar programming languages like $python3$ with the $psycopg2$ library, I was able to interface easily with the database and tune performance to my liking.  This allows for more in-depth and meaningful exploration of the actual data and concepts without extended ramp-up to the system.

\item \textbf{The System}

I examined a few parameters that are involved in tuning postgres performance.  They came in two flavors: Memory Parameters and Query Planner Parameters.

\begin{enumerate}{}
	\item Memory Parameters
	
	\begin{enumerate}
		\item \textit{work\_mem} - change performance of hash joins and sorts
	\end{enumerate}
	
	\item Query Planning Parameters
	
	\begin{enumerate}
		\item \textit{enable\_hashjoin} - Will want to find queries where a hash join is preferable and attempt to show performance of them improving or degrading.
		\item \textit{enable\_mergejoin} - Will want to find queries where a merge join is preferable and attempt to show performance of them improving or degrading.  Maybe create a hierarchy of join types?
		\item \textit{enable\_sort } - All sorting steps will be discouraged.  I would expect this to degrade performance significantly in ORDER BY and GROUP BY as well as sort-merge join queries.
		\item \textit{geqo\_effort} - This parameter sets the amount of time that postgres will spend looking for the ideal query plan.  It will be most effective in complicated queries (I suspect)
	\end{enumerate}

\end{enumerate}	

\item \textbf{The Experiments}

For these experiments, I will start with attempting a simplistic version of the test.  From there, I will move on to something more complicated and then finally perform the most intricate version of the test.  With each experiment I will measure the time taken for the query, join, insert, or update to execute.

\begin{enumerate}
	
	\item \textbf{Experiment 1: The 10\% Rule of Thumb}
	\begin{enumerate}
		\item This test explores when it is good to use an unclustered index vs. not using an index vs. using a clustered index
		\item Use a 100,000 tuple relation (scaled up version of TENKTUP1)
		\item Use Wisconsin Bench queries 2, 4 and 6. Run queries 2, 4, and 6 on the same dataset. Query 2 should be run without an index on unique2, Query 4 should be run with a clustered index on unique2, Query 6 requires an unclustered index on unique1
		\item No parameters changed in this test
		\item 
	\end{enumerate}
	 
	\item \textbf{Experiment 2: Joins}
	\begin{enumerate}
	 	\item This test aims to measure the performance of different types of joins across different relations using different algorithms.
	 	\item Use the 1ktup joined with 1ktup, 10ktup joined with 10ktup
	 	\item Run several joins - inner join on some predecate, left join, and joins not based on equality (join where id > 10)
	 	\item Change hashjoin and mergejoin allowed
	 	\item asdfasdfasdfasdfdsafdsa
	\end{enumerate}

 	\item \textbf{Experiment 3: Aggregation}
 	\begin{enumerate}
 		\item This test aims to measure the performance of aggregation with different parameters tuned to different values
 		\item Use an aggregation method GROUP BY with the 1k and 10k tup, with three separate columns: columns with unique values, columns with 1/2 values, columns with 1-100 values.  Run similar tests with GROUP BY HAVING
 		\item Queries will be based on the 1k 10k tables, with several predecates 
 		\item Enable\_sort, work\_mem
 		\item asdfasdfasdfasdfdsafdsa
 	\end{enumerate}

	\item \textbf{Experiment 4: Query Optimization}
	\begin{enumerate}
		\item For this experiment, I'd like to figure out just how much the query optimization time that postgres gives itself actually helps with performance.  I'd like to run a bunch of rounds of a few queries, each with the tuning set to different levels just to see if the improvement scales roughly linearly with the amount of time that the optimizer spends trying to optimize it.
		\item Because the optimizer is likely only used when there can be a variety of different joins, I'd like to create a three-way and four-way join between the 1k, 1k, 10k, and (another copy ofthe ) 10k different tables.
		\item Join queries, based on the ones from the Wisconsin benchmark.
		\item \textit{geqo\_effort} will be tuned accordingly.  There are several different integers corresponding to the amount of time that postgres will take trying to find the best possible query tree for a certain join.
		\item I expect that for simple queries that involve just a single two-way join, this number will not matter a whole lot.  However, as the amount of tables involved and the complexity of the joins themselves goes up, I suspect that this number will begin to be more and more important.
	\end{enumerate}
	 
\end{enumerate}

\item \textbf{Lessons Learned}
	
	\begin{enumerate}
	
	\item As always, I learned the lesson that I would hope that I learned each and every time that I do one of these projects: start earlier.  It is always more difficult to play catch up than it is to fine tune as you get further out in the timeframe.  Someone much smarter than me once told me of the 90/90 rule, where "the first 90\% of the project takes 90\% of the time, the last 10\% takes the other 90\% of the time."  This was the case in this project as well.
	
	\item Another less learned was to pick an option early and stick with it.  I spent a lot of time waffling between the two choices that I \textit{could've} made (comparing systems vs comparing parameters), and put a lot of initial research into both without firmly committing to either one.  When I first approached this assignment, I figured that by gathering all the data beforehand and then carefully weighing my options, I could make the absolute best decision.  What actually happened was that I ended up taking a large amount of time to consider two choices whose differences were ultimately not gigantic.  It would've been a much more effective use of time to simply pick one and stick to it.  Eventually I decided on testing the postgres parameters over Postgres Vs MySql because of a pretty inconsequential factor: the fact that I could work on postgres locally without an internet connection, and mySql would've required that I had access to the school's linuxlab machines.
	
	\end{enumerate}
	
\end{enumerate}	
\end{document}